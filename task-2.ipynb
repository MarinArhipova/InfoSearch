{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание №1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(urls):\n",
    "    links_tree = set()\n",
    "    for url in urls:\n",
    "        if len(texts) >= 101:\n",
    "            return\n",
    "    \n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "        \n",
    "        try:\n",
    "            #получаем контент страницы и декодируем его\n",
    "            html = requests.get(url).content.decode()\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        \n",
    "        #парсинг страницы\n",
    "        soup = BeautifulSoup(html)\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        text = soup.get_text(' ').strip().replace('\\xa0', ' ').replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "        text = re.sub(' +', ' ', text)\n",
    "    \n",
    "        if len(text.split(' ')) >= 101:\n",
    "            texts[url] = text\n",
    "            \n",
    "        parsed = urlparse(url)\n",
    "        base = f\"{parsed.scheme}://{parsed.netloc}\"    \n",
    "        links = re.findall('''<a\\s+(?:[^>]*?\\s+)?href=\"([^\"]*)\"''', html) \n",
    "        \n",
    "        for i, link in enumerate(links):    \n",
    "            if not urlparse(link).netloc:    \n",
    "                link_with_base = base + link    \n",
    "                links[i] = link_with_base\n",
    "                \n",
    "        links_tree.update(set(filter(lambda x: 'mailto' not in x, links)))\n",
    "    \n",
    "    crawler(links_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {}\n",
    "visited = set()\n",
    "start = 'https://m.habr.com/ru/all/'\n",
    "\n",
    "crawler([start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {i:k for i, k in enumerate(texts)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('index.txt', 'w') as f:\n",
    "    for i in index:\n",
    "        f.write(f\"{i}: {index[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"cat\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!cat index.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание №2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "import os, pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_all = {}\n",
    "lemma_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsed_list(word_list):\n",
    "    # получить список всех словоформ\n",
    "    morph, lst = pymorphy2.MorphAnalyzer(), []\n",
    "    for word in word_list:\n",
    "        lemma = \"\"\n",
    "        for x in morph.parse(word)[0].lexeme:\n",
    "            lemma+=x.word + \" \"\n",
    "        lst.append(f\"{word} {lemma}\\n\")\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_lemmatized = {}\n",
    "\n",
    "for url in texts:\n",
    "    text = texts[url]\n",
    "    # оставляем только слова\n",
    "    text = re.sub(r'[^a-zA-Zа-яА-Я]+', ' ', text)\n",
    "    \n",
    "    # удаляем повторяющиеся\n",
    "    words = \" \".join(sorted(set(text.lower().split()), key=text.lower().split().index))\n",
    "    words_all[url]=words\n",
    "    words = words.split(' ')\n",
    "    \n",
    "    lemma_words[url]=get_sparsed_list(words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in texts:\n",
    "    with open('words.txt', 'w') as f:\n",
    "        for i, score in enumerate(words):\n",
    "            f.write(f\"{score}\\n\")\n",
    "    with open('lemma_words.txt', 'w') as f:\n",
    "        for i, score in enumerate(lemma_words[url]):\n",
    "            f.write(f\"{score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
