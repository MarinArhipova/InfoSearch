{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание №1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(urls):\n",
    "    links_tree = set()\n",
    "    for url in urls:\n",
    "        if len(texts) >= 101:\n",
    "            return\n",
    "    \n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "        \n",
    "        try:\n",
    "            #получаем контент страницы и декодируем его\n",
    "            html = requests.get(url).content.decode()\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        \n",
    "        #парсинг страницы\n",
    "        soup = BeautifulSoup(html)\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        text = soup.get_text(' ').strip().replace('\\xa0', ' ').replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "        text = re.sub(' +', ' ', text)\n",
    "    \n",
    "        if len(text.split(' ')) >= 101:\n",
    "            texts[url] = text\n",
    "            \n",
    "        parsed = urlparse(url)\n",
    "        base = f\"{parsed.scheme}://{parsed.netloc}\"    \n",
    "        links = re.findall('''<a\\s+(?:[^>]*?\\s+)?href=\"([^\"]*)\"''', html) \n",
    "        \n",
    "        for i, link in enumerate(links):    \n",
    "            if not urlparse(link).netloc:    \n",
    "                link_with_base = base + link    \n",
    "                links[i] = link_with_base\n",
    "                \n",
    "        links_tree.update(set(filter(lambda x: 'mailto' not in x, links)))\n",
    "    \n",
    "    crawler(links_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {}\n",
    "visited = set()\n",
    "start = 'https://m.habr.com/ru/all/'\n",
    "\n",
    "crawler([start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {i:k for i, k in enumerate(texts)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('index.txt', 'w') as f:\n",
    "    for i in index:\n",
    "        f.write(f\"{i}: {index[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"cat\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!cat index.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание №2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "import os, pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_all = {}\n",
    "lemma_words = {}\n",
    "lemma_normal_forms = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_forms(word_list):\n",
    "    # получить список нормальных форм\n",
    "    morph, lst = pymorphy2.MorphAnalyzer(), []\n",
    "    return [morph.parse(word)[0].normal_form\n",
    "            for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsed_list(word_list):\n",
    "    # получить список всех словоформ\n",
    "    morph, lst = pymorphy2.MorphAnalyzer(), []\n",
    "    for word in word_list:\n",
    "        lemma = \"\"\n",
    "        for x in morph.parse(word)[0].lexeme:\n",
    "            lemma+=x.word + \" \"\n",
    "        lst.append(f\"{word} {lemma}\\n\")\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "texts_lemmatized = {}\n",
    "\n",
    "for url in texts:\n",
    "    text = texts[url]\n",
    "    # оставляем только слова\n",
    "    text = re.sub(r'[^a-zA-Zа-яА-Я]+', ' ', text)\n",
    "   \n",
    "    words = text.split(' ')\n",
    "    words_all[url]=words\n",
    "    \n",
    "    lemma_words[url]=get_sparsed_list(words_all[url])  \n",
    "    lemma_normal_forms[url]=get_normal_forms(words_all[url])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('words.txt', 'w') as f:\n",
    "    for url in words_all:\n",
    "        for i, score in enumerate(words_all[url]):\n",
    "            f.write(f\"{score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemma_words.txt', 'w') as f:\n",
    "    for url in lemma_words:\n",
    "        for i, score in enumerate(lemma_words[url]):\n",
    "            f.write(f\"{score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание №3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [index[k] for k in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_index = {}\n",
    "\n",
    "for url in lemma_normal_forms:\n",
    "    for word in lemma_normal_forms[url]:\n",
    "        if not word in reverse_index:\n",
    "            reverse_index[word] = set([url])\n",
    "        else:\n",
    "            reverse_index[word].update([url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic = {\n",
    "    '&': lambda x,y: x & y,\n",
    "    '|': lambda x,y: x | y,\n",
    "    '!': lambda x: set([doc for doc in docs if doc not in x])\n",
    "}\n",
    "\n",
    "def find(query):\n",
    "    query = query.replace('(', ' ( ').replace(')', ' ) ').replace('!','! ').split()\n",
    "    \n",
    "    indexes_words = [i for i,x in enumerate(query) if not x in ['!', '&', '|']]\n",
    "    for i in indexes_words:\n",
    "        query[i] = reverse_index[query[i]]\n",
    "    \n",
    "    indexes_not = [i for i,x in enumerate(query) if x == '!']\n",
    "    for i in indexes_not:\n",
    "        x = query[i+1]\n",
    "        query[i+1] = logic['!'](x)\n",
    "        query.pop(i)\n",
    "    \n",
    "    indexes_and = [i for i,x in enumerate(query) if x == '&']\n",
    "    for i in indexes_and:\n",
    "        x = query[i-1]\n",
    "        y = query[i+1]\n",
    "        query[i-1] = logic['&'](x,y)\n",
    "        query.pop(i)\n",
    "        query.pop(i)\n",
    "    \n",
    "    indexes_or = [i for i,x in enumerate(query) if x == '|']\n",
    "    for i in indexes_or:\n",
    "        x = query[i-1]\n",
    "        y = query[i+1]\n",
    "        query[i-1] = logic['|'](x,y)\n",
    "        query.pop(i)\n",
    "        query.pop(i)\n",
    "    \n",
    "    return query[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = random.choice(list(reverse_index.keys()))\n",
    "x2 = random.choice(list(reverse_index.keys()))\n",
    "x3 = random.choice(list(reverse_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "полоска | снижать & !itnews\n"
     ]
    }
   ],
   "source": [
    "query = f'{x1} | {x2} & !{x3}'\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://m.habr.com/ru/company/oleg-bunin/blog/543386/',\n",
       " 'https://m.habr.com/ru/company/ruvds/profile/',\n",
       " 'https://m.habr.com/ru/post/409639/'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
