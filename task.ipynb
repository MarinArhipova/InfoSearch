{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# re - модуль для работы с регулярными выражениями\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание №1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(urls):\n",
    "    links_tree = set()\n",
    "    for url in urls:\n",
    "        if len(texts) >= 101:\n",
    "            return\n",
    "    \n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "        \n",
    "        try:\n",
    "            #получаем контент страницы и декодируем его\n",
    "            html = requests.get(url).content.decode()\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        \n",
    "        #парсинг страницы\n",
    "        soup = BeautifulSoup(html)\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        text = soup.get_text(' ').strip().replace('\\xa0', ' ').replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "        # re.sub() для замены всех ' +' на ' '\n",
    "        text = re.sub(' +', ' ', text)\n",
    "    \n",
    "        if len(text.split(' ')) >= 101:\n",
    "            texts[url] = text\n",
    "            \n",
    "        parsed = urlparse(url)\n",
    "        base = f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "        # re.findall() - найти все вхождения\n",
    "        links = re.findall('''<a\\s+(?:[^>]*?\\s+)?href=\"([^\"]*)\"''', html) \n",
    "        \n",
    "        for i, link in enumerate(links):    \n",
    "            if not urlparse(link).netloc:    \n",
    "                link_with_base = base + link    \n",
    "                links[i] = link_with_base\n",
    "                \n",
    "        links_tree.update(set(filter(lambda x: 'mailto' not in x, links)))\n",
    "    \n",
    "    crawler(links_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {}\n",
    "visited = set()\n",
    "start = 'https://m.habr.com/ru/all/'\n",
    "\n",
    "crawler([start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {i:k for i, k in enumerate(texts)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('index.txt', 'w') as f:\n",
    "    for i in index:\n",
    "        f.write(f\"{i}: {index[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0: https://m.habr.com/ru/all/\\n1: https://m.habr.com/ru/flows/marketing/\\n2: https://m.habr.com/ru/company/skillfactory/profile/\\n3: https://m.habr.com/ru/users/cointegrated/\\n4: https://account.habr.com/info/agreement\\n5: https://m.habr.com/ru/feedback/\\n6: https://m.habr.com/ru/post/540210/comments/\\n7: https://m.habr.com/ru/news/t/543604/comments/\\n8: https://m.habr.com/ru/company/selectel/profile/\\n9: https://m.habr.com/ru/users/\\n10: https://m.habr.com/ru/hubs/\\n11: https://m.habr.com/ru/users/ruvds/\\n12: https://m.habr.com/ru/users/val9670/\\n13: https://m.habr.com/ru/post/543686/\\n14: https://m.habr.com/ru/docs/companies/corpblogs/\\n15: https://m.habr.com/ru/post/540210/\\n16: https://m.habr.com/ru/company/otus/profile/\\n17: https://m.habr.com/ru/users/Old_tutor/\\n18: https://m.habr.com/ru/company/jugru/profile/\\n19: https://m.habr.com/ru/company/kaspersky/blog/543690/comments/\\n20: https://account.habr.com/info/confidential/\\n21: https://m.habr.com/ru/company/cloudera/blog/543652/comments/\\n22: https://m.habr.com/ru/post/543678/comments/\\n23: https://tmtm.ru/services/content/\\n24: https://m.habr.com/ru/news/t/543604/\\n25: https://m.habr.com/ru/users/K-ILYA-V/\\n26: https://m.habr.com/ru/company/ruvds/profile/\\n27: https://m.habr.com/ru/news/t/543624/comments/\\n28: https://m.habr.com/ru/company/audiomania/blog/540790/\\n29: https://m.habr.com/ru/users/subvp/\\n30: https://m.habr.com/ru/post/543680/\\n31: https://m.habr.com/ru/company/vdsina/profile/\\n32: https://m.habr.com/ru/company/kaspersky/blog/543690/\\n33: https://m.habr.com/ru/flows/popsci/\\n34: https://m.habr.com/ru/news/t/543674/comments/\\n35: https://m.habr.com/ru/users/Leo_Gan/\\n36: https://m.habr.com/ru/post/543648/comments/\\n37: https://m.habr.com/ru/post/543696/comments/\\n38: https://m.habr.com/ru/post/543694/\\n39: https://m.habr.com/ru/company/skillfactory/blog/542862/\\n40: https://tmtm.ru/services/advertising/\\n41: https://m.habr.com/ru/users/Audioman/\\n42: https://tmtm.ru/workshops/\\n43: https://m.habr.com/ru/megaprojects/\\n44: https://m.habr.com/ru/post/543680/comments/\\n45: https://m.habr.com/ru/post/543650/\\n46: https://m.habr.com/ru/company/audiomania/blog/540790/comments/\\n47: https://m.habr.com/ru/post/543500/\\n48: https://m.habr.com/ru/news/\\n49: https://m.habr.com/ru/post/543694/comments/\\n50: https://m.habr.com/ru/news/t/543624/\\n51: https://m.habr.com/ru/post/543696/\\n52: https://vk.com/habr\\n53: https://m.habr.com/ru/post/543678/\\n54: https://m.habr.com/ru/all/page3/\\n55: https://m.habr.com/ru/company/itsoft/profile/\\n56: https://m.habr.com/ru/company/skillfactory/blog/542862/comments/\\n57: https://m.habr.com/ru/users/Kaspersky_Lab/\\n58: https://m.habr.com/ru/flows/develop/\\n59: https://m.habr.com/ru/company/itsoft/blog/543554/comments/\\n60: https://m.habr.com/ru/post/543702/\\n61: https://m.habr.com/ru/docs/docs/transparency/\\n62: https://m.habr.com/ru/post/543698/\\n63: https://www.facebook.com/habrahabr.ru\\n64: https://m.habr.com/ru/news/t/543656/comments/\\n65: https://m.habr.com/ru/company/itsumma/profile/\\n66: https://m.habr.com/ru/companies/\\n67: https://m.habr.com/ru/all/page49/\\n68: https://m.habr.com/ru/company/itsoft/blog/543668/\\n69: https://m.habr.com/ru/post/543614/\\n70: https://m.habr.com/ru/company/itsoft/blog/543662/\\n71: https://m.habr.com/ru/users/ITSoftWeb/\\n72: https://u.habr.com/home-admin-header\\n73: https://tmtm.ru/services/corpblog/\\n74: https://m.habr.com/ru/news/t/543674/\\n75: https://m.habr.com/ru/post/543686/comments/\\n76: https://m.habr.com/ru/users/Tzimie/\\n77: https://m.habr.com/ru/company/cloudera/blog/543652/\\n78: https://m.habr.com/ru/all/page50/\\n79: https://m.habr.com/ru/news/t/543600/comments/\\n80: https://m.habr.com/ru/company/timeweb/blog/543642/comments/\\n81: https://company.habr.com/\\n82: https://m.habr.com/ru/\\n83: https://m.habr.com/ru/company/ruvds/blog/543640/comments/\\n84: https://m.habr.com/ru/users/StjarnornasFred/\\n85: https://m.habr.com/ru/company/itsoft/blog/543668/comments/\\n86: https://m.habr.com/ru/company/timeweb/blog/543642/\\n87: https://m.habr.com/ru/post/543698/comments/\\n88: https://m.habr.com/ru/news/t/543600/\\n89: https://m.habr.com/ru/news/t/543656/\\n90: https://m.habr.com/ru/company/itsoft/blog/543662/comments/\\n91: https://m.habr.com/ru/company/ruvds/blog/543640/\\n92: https://m.habr.com/ru/company/pvs-studio/profile/\\n93: https://m.habr.com/ru/company/itsoft/blog/543554/\\n94: https://m.habr.com/ru/users/fokus-lop/\\n95: https://m.habr.com/ru/users/afedintsev/\\n96: https://m.habr.com/ru/post/543614/comments/\\n97: https://m.habr.com/ru/post/543648/\\n98: https://m.habr.com/ru/post/543650/comments/\\n99: https://m.habr.com/ru/post/543500/comments/\\n100: https://m.habr.com/ru/company/mailru/profile/\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'C:\\\\Users\\\\User\\\\Downloads\\\\InfoSearch\\\\InfoSearch_1\\\\index.txt'  # путь указывается индивидуально\n",
    "open(path).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание №2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "import os, pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pymystem3.Mystem — морфологический анализатор русского языка с поддержкой снятия морфологической неоднозначности, \n",
    "# разработанный компанией «Яндекс». Программа работает на основе словаря и способна формировать морфологические гипотезы \n",
    "# о незнакомых словах\n",
    "lemmatizer = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_all = {}\n",
    "lemma_words = {}\n",
    "lemma_word_forms = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsed_list(word_list):\n",
    "    # получить список всех словоформ\n",
    "    morph, lst = pymorphy2.MorphAnalyzer(), []\n",
    "    for word in word_list:\n",
    "        lemma = \"\"\n",
    "        for x in morph.parse(word)[0].lexeme:\n",
    "            lemma+=x.word + \" \"\n",
    "        lst.append(f\"{word} {lemma}\\n\")\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in texts:\n",
    "    text = texts[url]\n",
    "    # оставляем только слова\n",
    "    text = re.sub(r'[^a-zA-Zа-яА-Я]+', ' ', text)\n",
    "    \n",
    "    # удаляем повторяющиеся\n",
    "    words = \" \".join(sorted(set(text.lower().split()), key=text.lower().split().index))\n",
    "    words_all[url]=words\n",
    "    words = words.split(' ')\n",
    "    \n",
    "    lemmas = lemmatizer.lemmatize(words_all[url])\n",
    "    lemma_words[url] = ''.join(lemmas).strip()\n",
    "    \n",
    "    lemma_word_forms[url]=get_sparsed_list(words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in texts:\n",
    "    with open('words.txt', 'w') as f:\n",
    "        for i, score in enumerate(words):\n",
    "            f.write(f\"{score}\\n\")\n",
    "    with open('lemma_words.txt', 'w') as f:\n",
    "        for i, score in enumerate(lemma_word_forms[url]):\n",
    "            f.write(f\"{score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание №3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [index[k] for k in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание инвертированного списка терминов (индекс)\n",
    "\n",
    "reverse_index = {}\n",
    "\n",
    "for url in lemma_words:\n",
    "    # пройдемся по списку лемматизированных токенов\n",
    "    for word in lemma_words[url].split(' '):\n",
    "        # если слово не входит в reverse_index, то добавляем его туда\n",
    "        if not word in reverse_index:\n",
    "            reverse_index[word] = set([url])\n",
    "        else:\n",
    "            reverse_index[word].update([url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# булев поиск по построенному индексу\n",
    "\n",
    "logic = {\n",
    "    '&': lambda x,y: x & y,\n",
    "    '|': lambda x,y: x | y,\n",
    "    '!': lambda x: set([doc for doc in docs if doc not in x])\n",
    "}\n",
    "\n",
    "def find(query):\n",
    "    query = query.replace('(', ' ( ').replace(')', ' ) ').replace('!','! ').split()\n",
    "    \n",
    "    # слова, которые должны присутствовать на странице\n",
    "    indexes_words = [i for i,x in enumerate(query) if not x in ['!', '&', '|']]\n",
    "    for i in indexes_words:\n",
    "        query[i] = reverse_index[query[i]]\n",
    "    \n",
    "    # слова, которые не должны присутствовать на странице\n",
    "    indexes_not = [i for i,x in enumerate(query) if x == '!']\n",
    "    for i in indexes_not:\n",
    "        x = query[i+1]\n",
    "        query[i+1] = logic['!'](x)\n",
    "        query.pop(i)\n",
    "    \n",
    "    # фразы, которые должны присутствовать на странице\n",
    "    indexes_and = [i for i,x in enumerate(query) if x == '&']\n",
    "    for i in indexes_and:\n",
    "        x = query[i-1]\n",
    "        y = query[i+1]\n",
    "        query[i-1] = logic['&'](x,y)\n",
    "        # list.pop([i]) - Возвращает элемент [на указанной позиции], удаляя его из списка.\n",
    "        query.pop(i)\n",
    "        query.pop(i)\n",
    "    \n",
    "    # хотя бы одно из указанных тут слов должно присутствовать на странице\n",
    "    indexes_or = [i for i,x in enumerate(query) if x == '|']\n",
    "    for i in indexes_or:\n",
    "        x = query[i-1]\n",
    "        y = query[i+1]\n",
    "        query[i-1] = logic['|'](x,y)\n",
    "        query.pop(i)\n",
    "        query.pop(i)\n",
    "    \n",
    "    return query[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.choice() - возвращает случайный элемент из указанной последовательности\n",
    "x1 = random.choice(list(reverse_index.keys()))\n",
    "x2 = random.choice(list(reverse_index.keys()))\n",
    "x3 = random.choice(list(reverse_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset | экономически & !zangasta\n"
     ]
    }
   ],
   "source": [
    "query = f'{x1} | {x2} & !{x3}'\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://m.habr.com/ru/company/itsoft/blog/543662/comments/',\n",
       " 'https://m.habr.com/ru/company/skillfactory/blog/542862/',\n",
       " 'https://m.habr.com/ru/post/543702/'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
